{
  "top_sid": "v3_commit_7501050_79",
  "generated_file": "train_20260210_092816.py",
  "applied_diff": "--- top\n+++ improved\n@@ -1,5 +1,3 @@\n-\"Amil Don't please copy\"\n\n-\n\n import gc\n\n import os\n\n from dataclasses import dataclass\n\n@@ -10,37 +8,20 @@\n \n\n @dataclass\n\n class InnerStepsResult:\n\n-    \"\"\"Result from inner_steps training loop.\"\"\"\n\n     final_logits: torch.Tensor\n\n     total_tokens: int\n\n     final_loss: float\n\n \n\n \n\n-# Global state \u2014 persists across inner_steps calls (warmup + eval)\n\n _initialized = False\n\n _train_fn = None\n\n \n\n \n\n def inner_steps(model, data_iterator, optimizer, num_steps, device):\n\n-    \"\"\"Optimized training loop for maximum TPS on validator.\n\n-\n\n-    Args:\n\n-        model: HuggingFace model\n\n-        data_iterator: Yields batches of token ids\n\n-        optimizer: AdamW or GradientCapturingOptimizer wrapper\n\n-        num_steps: Number of training steps\n\n-        device: CUDA device\n\n-\n\n-    Returns:\n\n-        InnerStepsResult with final_logits (batch, seq_len-1, vocab), total_tokens, final_loss\n\n-    \"\"\"\n\n     global _initialized, _train_fn\n\n \n\n     is_cuda = str(device).startswith(\"cuda\")\n\n \n\n-    # =========================================================================\n\n-    # ONE-TIME INIT \u2014 runs once across all inner_steps calls\n\n-    # =========================================================================\n\n     if not _initialized:\n\n         _initialized = True\n\n \n\n@@ -53,28 +34,37 @@\n             torch.backends.cudnn.deterministic = False\n\n             torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n\n \n\n-            # Force flash SDP, disable math SDP (slow fallback)\n\n             torch.backends.cuda.enable_flash_sdp(True)\n\n             torch.backends.cuda.enable_mem_efficient_sdp(True)\n\n             torch.backends.cuda.enable_math_sdp(False)\n\n \n\n-            # Free reference optimizer from verify.py / env.py (~24GB VRAM)\n\n+            try:\n\n+                import torch._inductor.config as inductor_config\n\n+                inductor_config.coordinate_descent_tuning = True\n\n+                inductor_config.triton.unique_kernel_names = True\n\n+                inductor_config.fx_graph_cache = True\n\n+            except Exception:\n\n+                pass\n\n+\n\n+            try:\n\n+                import torch._dynamo.config as dynamo_config\n\n+                dynamo_config.automatic_dynamic_shapes = False\n\n+                dynamo_config.assume_static_by_default = True\n\n+            except Exception:\n\n+                pass\n\n+\n\n             for obj in gc.get_objects():\n\n                 if isinstance(obj, torch.optim.Optimizer) and obj is not optimizer:\n\n                     obj.state.clear()\n\n             gc.collect()\n\n             torch.cuda.empty_cache()\n\n \n\n-    # =========================================================================\n\n-    # PER-CALL SETUP \u2014 lightweight, runs each inner_steps call\n\n-    # =========================================================================\n\n     if hasattr(model, \"config\"):\n\n         model.config.use_cache = False\n\n \n\n     if hasattr(model, \"gradient_checkpointing_disable\"):\n\n         model.gradient_checkpointing_disable()\n\n \n\n-    # Fused optimizer \u2014 single CUDA kernel for param update (use provided optimizer only)\n\n     if is_cuda:\n\n         try:\n\n             for group in optimizer.param_groups:\n\n@@ -84,10 +74,6 @@\n         except Exception:\n\n             pass\n\n \n\n-    # =========================================================================\n\n-    # COMPILED TRAIN STEP \u2014 cached globally across calls\n\n-    # Returns (loss, logits) so we can return final_logits for verification.\n\n-    # =========================================================================\n\n     if _train_fn is None:\n\n         dev_type = device.type\n\n \n\n@@ -106,7 +92,7 @@\n             try:\n\n                 _train_fn = torch.compile(\n\n                     _eager_step,\n\n-                    mode=\"reduce-overhead\",\n\n+                    mode=\"max-autotune\",\n\n                     fullgraph=False,\n\n                 )\n\n             except Exception:\n\n@@ -114,49 +100,48 @@\n         else:\n\n             _train_fn = _eager_step\n\n \n\n-    # =========================================================================\n\n-    # PREFETCH FIRST BATCH\n\n-    # =========================================================================\n\n-    if is_cuda:\n\n-        pf_stream = torch.cuda.Stream()\n\n-        with torch.cuda.stream(pf_stream):\n\n-            next_batch = next(data_iterator).to(device, non_blocking=True)\n\n-    else:\n\n-        next_batch = next(data_iterator).to(device)\n\n-\n\n-    # =========================================================================\n\n-    # TRAINING LOOP\n\n-    # =========================================================================\n\n-    total_tokens = 0\n\n-    final_loss = 0.0\n\n-    final_logits = None\n\n-\n\n-    for step in range(num_steps):\n\n-        # Wait for prefetched batch\n\n+    gc_was_enabled = gc.isenabled()\n\n+    if gc_was_enabled:\n\n+        gc.disable()\n\n+\n\n+    try:\n\n         if is_cuda:\n\n-            torch.cuda.current_stream().wait_stream(pf_stream)\n\n-        batch = next_batch\n\n-\n\n-        # Prefetch next batch (overlap with compute)\n\n-        if step < num_steps - 1:\n\n-            nb = next(data_iterator)\n\n+            pf_stream = torch.cuda.Stream()\n\n+            with torch.cuda.stream(pf_stream):\n\n+                next_batch = next(data_iterator).to(device, non_blocking=True)\n\n+        else:\n\n+            next_batch = next(data_iterator).to(device)\n\n+\n\n+        total_tokens = 0\n\n+        final_loss = 0.0\n\n+        final_logits = None\n\n+\n\n+        for step in range(num_steps):\n\n             if is_cuda:\n\n-                with torch.cuda.stream(pf_stream):\n\n-                    next_batch = nb.to(device, non_blocking=True)\n\n-            else:\n\n-                next_batch = nb.to(device)\n\n-\n\n-        # Forward + backward (returns loss, logits)\n\n-        loss, logits = _train_fn(batch[:, :-1], batch[:, 1:])\n\n-\n\n-        # Must use provided optimizer every step (no access to optimizer internals)\n\n-        optimizer.step()\n\n-        optimizer.zero_grad(set_to_none=True)\n\n-\n\n-        total_tokens += batch.numel()\n\n-        if step == num_steps - 1:\n\n-            final_loss = loss.item()\n\n-            final_logits = logits.float()\n\n+                torch.cuda.current_stream().wait_stream(pf_stream)\n\n+            batch = next_batch\n\n+\n\n+            if step < num_steps - 1:\n\n+                nb = next(data_iterator)\n\n+                if is_cuda:\n\n+                    with torch.cuda.stream(pf_stream):\n\n+                        next_batch = nb.to(device, non_blocking=True)\n\n+                else:\n\n+                    next_batch = nb.to(device)\n\n+\n\n+            loss, logits = _train_fn(batch[:, :-1], batch[:, 1:])\n\n+\n\n+            optimizer.step()\n\n+            optimizer.zero_grad(set_to_none=True)\n\n+\n\n+            total_tokens += batch.numel()\n\n+            if step == num_steps - 1:\n\n+                final_loss = loss.item()\n\n+                final_logits = logits.float()\n\n+\n\n+    finally:\n\n+        if gc_was_enabled:\n\n+            gc.enable()\n\n \n\n     return InnerStepsResult(\n\n         final_logits=final_logits,\n\n@@ -165,9 +150,6 @@\n     )\n\n \n\n \n\n-# =============================================================================\n\n-# LOCAL TESTING\n\n-# =============================================================================\n\n if __name__ == \"__main__\":\n\n     import json\n\n     import time\n\n@@ -176,7 +158,7 @@\n     from transformers import AutoModelForCausalLM\n\n \n\n     print(\"=\" * 60)\n\n-    print(\"TESTING train.py - Wrapper Bypass + Compile (Cached)\")\n\n+    print(\"TESTING train_agent_output.py\")\n\n     print(\"=\" * 60)\n\n     print()\n\n \n\n@@ -211,7 +193,7 @@\n \n\n     attn_impl = \"sdpa\"\n\n     try:\n\n-        import flash_attn  # noqa: F401\n\n+        import flash_attn\n\n         attn_impl = \"flash_attention_2\"\n\n     except ImportError:\n\n         pass\n\n@@ -255,7 +237,7 @@\n     if torch.cuda.is_available():\n\n         torch.cuda.synchronize()\n\n     t1 = time.perf_counter()\n\n-    pr\n... (truncated)",
  "saved_at": "2026-02-10T09:28:16.645952"
}