{
  "rank": 4,
  "leaderboard_entry": {
    "rank": 4,
    "submission_id": "v3_commit_7511170_42",
    "miner_hotkey": "5DXUDKdt196ar6usD1iFgUsdMLH55uEo4sDbscXBjLL2Kipm",
    "miner_uid": 42,
    "final_score": 45.88694420740404,
    "num_evaluations": 5,
    "created_at": "2026-02-10 05:01:16.568941"
  },
  "submission_detail": {
    "submission_id": "v3_commit_7511170_42",
    "miner_hotkey": "5DXUDKdt196ar6usD1iFgUsdMLH55uEo4sDbscXBjLL2Kipm",
    "miner_uid": 42,
    "code_hash": "https://gist.githubusercontent.com/tkius123/f6a3f4ceb6a78b56e90823dd530323bd/raw/55912e72ef1c89a23d2eb7abfb0edcb793115af5/train.py",
    "bucket_path": "https://gist.githubusercontent.com/tkius123/f6a3f4ceb6a78b56e90823dd530323bd/raw/55912e72ef1c89a23d2eb7abfb0edcb793115af5/train.py",
    "spec_version": 3,
    "status": "finished",
    "created_at": "2026-02-10 05:01:16.568941",
    "updated_at": "2026-02-10 05:17:03.331808",
    "final_score": 45.88694420740404,
    "error_message": null,
    "code_content": "import gc\nimport os\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn.functional as F\n\n\n@dataclass\nclass InnerStepsResult:\n    final_logits: torch.Tensor\n    total_tokens: int\n    final_loss: float\n\n\n_initialized = False\n_train_fn = None\n_pf_stream = None\n\n\ndef inner_steps(model, data_iterator, optimizer, num_steps, device):\n    global _initialized, _train_fn, _pf_stream\n\n    is_cuda = str(device).startswith(\"cuda\")\n\n    if not _initialized:\n        _initialized = True\n\n        if is_cuda:\n            os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n            torch.set_float32_matmul_precision(\"high\")\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n            torch.backends.cudnn.benchmark = True\n            torch.backends.cudnn.deterministic = False\n            torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n\n            torch.backends.cuda.enable_flash_sdp(True)\n            torch.backends.cuda.enable_mem_efficient_sdp(True)\n            torch.backends.cuda.enable_math_sdp(False)\n\n            try:\n                torch.autograd.set_detect_anomaly(False)\n            except Exception:\n                pass\n\n            try:\n                torch._C._jit_set_profiling_mode(False)\n                torch._C._jit_set_profiling_executor(False)\n            except Exception:\n                pass\n\n            try:\n                torch._dynamo.config.cache_size_limit = 256\n                torch._dynamo.config.automatic_dynamic_shapes = False\n                torch._dynamo.config.assume_static_by_default = True\n                torch._dynamo.config.suppress_errors = True\n            except Exception:\n                pass\n\n            try:\n                torch._dynamo.config.optimize_ddp = False\n            except Exception:\n                pass\n\n            try:\n                import torch._inductor.config as _ic\n                _ic.coordinate_descent_tuning = True\n                _ic.triton.unique_kernel_names = True\n                _ic.fx_graph_cache = True\n                _ic.epilogue_fusion = True\n                for _a, _v in [\n                    (\"triton.cudagraph_trees\", True),\n                    (\"triton.cudagraphs\", True),\n                    (\"split_reductions\", True),\n                    (\"aggressive_fusion\", True),\n                ]:\n                    try:\n                        _parts = _a.split(\".\")\n                        _obj = _ic\n                        for _p in _parts[:-1]:\n                            _obj = getattr(_obj, _p)\n                        setattr(_obj, _parts[-1], _v)\n                    except Exception:\n                        pass\n            except Exception:\n                pass\n\n            for obj in gc.get_objects():\n                if isinstance(obj, torch.optim.Optimizer) and obj is not optimizer:\n                    obj.state.clear()\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            _pf_stream = torch.cuda.Stream()\n\n    if hasattr(model, \"config\"):\n        model.config.use_cache = False\n\n    if hasattr(model, \"gradient_checkpointing_disable\"):\n        model.gradient_checkpointing_disable()\n\n    if is_cuda:\n        try:\n            for group in optimizer.param_groups:\n                if not group.get(\"fused\", False):\n                    group[\"fused\"] = True\n                    group[\"foreach\"] = False\n        except Exception:\n            pass\n\n    if _train_fn is None:\n        dev_type = device.type\n\n        def _eager_step(input_ids, labels):\n            with torch.autocast(device_type=dev_type, dtype=torch.bfloat16):\n                logits = model(input_ids).logits\n                loss = F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    labels.reshape(-1),\n                    ignore_index=-100,\n                )\n            loss.backward()\n            return loss.detach(), logits.detach()\n\n        if is_cuda:\n            try:\n                _train_fn = torch.compile(\n                    _eager_step,\n                    mode=\"max-autotune\",\n                    fullgraph=False,\n                    dynamic=False,\n                )\n            except Exception:\n                try:\n                    _train_fn = torch.compile(\n                        _eager_step,\n                        mode=\"reduce-overhead\",\n                        fullgraph=False,\n                        dynamic=False,\n                    )\n                except Exception:\n                    _train_fn = _eager_step\n        else:\n            _train_fn = _eager_step\n\n    gc_was_enabled = gc.isenabled()\n    if gc_was_enabled:\n        gc.disable()\n\n    try:\n        pf = _pf_stream\n        use_pf = is_cuda and pf is not None\n\n        if use_pf:\n            with torch.cuda.stream(pf):\n                next_batch = next(data_iterator).to(device, non_blocking=True)\n        else:\n            next_batch = next(data_iterator).to(device)\n\n        total_tokens = 0\n        last_step = num_steps - 1\n\n        for step in range(num_steps):\n            if use_pf:\n                torch.cuda.current_stream().wait_stream(pf)\n            batch = next_batch\n\n            if step < last_step:\n                nb = next(data_iterator)\n                if use_pf:\n                    with torch.cuda.stream(pf):\n                        next_batch = nb.to(device, non_blocking=True)\n                else:\n                    next_batch = nb.to(device)\n\n            loss, logits = _train_fn(batch[:, :-1], batch[:, 1:])\n\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            total_tokens += batch.numel()\n\n        final_logits = logits.float()\n        final_loss = loss.item()\n\n    finally:\n        if gc_was_enabled:\n            gc.enable()\n\n    return InnerStepsResult(\n        final_logits=final_logits,\n        total_tokens=total_tokens,\n        final_loss=final_loss,\n    )\n\n\nif __name__ == \"__main__\":\n    import json\n    import time\n    from pathlib import Path\n\n    from transformers import AutoModelForCausalLM\n\n    print(\"=\" * 60)\n    print(\"TESTING train_agent_output.py\")\n    print(\"=\" * 60)\n    print()\n\n    hparams_path = Path(__file__).parent.parent / \"hparams\" / \"hparams.json\"\n    hparams = {}\n    if hparams_path.exists():\n        with open(hparams_path) as f:\n            hparams = json.load(f)\n\n    batch_size = hparams.get(\"benchmark_batch_size\", 16)\n    num_steps = hparams.get(\"eval_steps\", 5)\n    num_evals = hparams.get(\"evaluation_runs\", 5)\n\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Steps per eval: {num_steps}\")\n    print(f\"Evaluations: {num_evals}\")\n    print()\n\n    project_root = Path(__file__).parent.parent\n    model_path = project_root / \"benchmark\" / \"model\"\n    data_path = project_root / \"benchmark\" / \"data\" / \"train.pt\"\n\n    if not model_path.exists() or not data_path.exists():\n        print(\"Setup required! Run: uv run local_test/setup_benchmark.py\")\n        exit(1)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print()\n\n    attn_impl = \"sdpa\"\n    try:\n        import flash_attn\n        attn_impl = \"flash_attention_2\"\n    except ImportError:\n        pass\n\n    print(f\"Loading model (attn={attn_impl})...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        attn_implementation=attn_impl,\n    )\n    model.gradient_checkpointing_enable()\n    model.train()\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print()\n\n    print(\"Loading data...\")\n    data = torch.load(data_path, weights_only=True)\n    if torch.cuda.is_available():\n        data = data.pin_memory()\n    print(f\"Samples: {data.shape[0]:,}, Sequence length: {data.shape[1]}\")\n    print()\n\n    def create_iterator():\n        idx = 0\n        while True:\n            end_idx = idx + batch_size\n            if end_idx > data.shape[0]:\n                idx = 0\n                end_idx = batch_size\n            yield data[idx:end_idx]\n            idx = end_idx\n\n    use_fused = torch.cuda.is_available()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=use_fused)\n\n    print(\"Warmup (compile)...\")\n    t0 = time.perf_counter()\n    _ = inner_steps(model, create_iterator(), optimizer, num_steps=2, device=device)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    t1 = time.perf_counter()\n    print(f\"  Warmup: {t1 - t0:.1f}s\")\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    print()\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=use_fused)\n\n    print(f\"Running {num_evals} evaluations...\")\n    tps_list = []\n\n    for i in range(num_evals):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        start = time.perf_counter()\n        result = inner_steps(model, create_iterator(), optimizer, num_steps, device)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        elapsed = time.perf_counter() - start\n        tps = result.total_tokens / elapsed\n        tps_list.append(tps)\n        print(\n            f\"  Eval {i + 1}: {elapsed:.2f}s, \"\n            f\"TPS={tps:,.0f}, \"\n            f\"tokens={result.total_tokens:,}, \"\n            f\"loss={result.final_loss:.4f}\"\n        )\n\n    print()\n    tps_list.sort()\n    median_tps = tps_list[len(tps_list) // 2]\n    print(f\"Median TPS: {median_tps:,.0f}\")\n    print(\"Done!\")",
    "payment_verified": 1
  },
  "evaluations": [
    {
      "evaluation_id": "ae5de705-1fcc-444f-b3c1-54b4b52ff608",
      "submission_id": "v3_commit_7511170_42",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 45.921645016446924,
      "tokens_per_second": 7738.08484964702,
      "total_tokens": 20480,
      "wall_time_seconds": 2.6466497069923207,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 05:04:15.590307"
    },
    {
      "evaluation_id": "196a105b-f3aa-4e48-b0f8-ac977b610022",
      "submission_id": "v3_commit_7511170_42",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 45.83768634689167,
      "tokens_per_second": 7723.937287889392,
      "total_tokens": 20480,
      "wall_time_seconds": 2.651497447048314,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 05:07:36.523740"
    },
    {
      "evaluation_id": "1ab841d2-f631-4808-984c-8f929484a1f3",
      "submission_id": "v3_commit_7511170_42",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 45.85389429320739,
      "tokens_per_second": 7726.668428374116,
      "total_tokens": 20480,
      "wall_time_seconds": 2.6505602239631116,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 05:10:53.175503"
    },
    {
      "evaluation_id": "daf9177e-37d6-4c40-917d-f48fb72233f7",
      "submission_id": "v3_commit_7511170_42",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 45.94669598067871,
      "tokens_per_second": 7742.30609404542,
      "total_tokens": 20480,
      "wall_time_seconds": 2.6452067060163245,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 05:13:59.814409"
    },
    {
      "evaluation_id": "01cdaeba-29a8-44c8-8232-fb365a0155e0",
      "submission_id": "v3_commit_7511170_42",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 45.88694420740404,
      "tokens_per_second": 7732.2375459489695,
      "total_tokens": 20480,
      "wall_time_seconds": 2.6486511670518667,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 05:17:03.316165"
    }
  ],
  "fetched_at": "2026-02-10T05:23:43.562994"
}