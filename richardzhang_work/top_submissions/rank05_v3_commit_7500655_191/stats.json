{
  "rank": 5,
  "leaderboard_entry": {
    "rank": 5,
    "submission_id": "v3_commit_7500655_191",
    "miner_hotkey": "5DCsaYaL5VnvnMVRbuTojQFJafXVxuVucAbsPM9XQZATZp83",
    "miner_uid": 191,
    "final_score": 44.444478109007605,
    "num_evaluations": 9,
    "created_at": "2026-02-08 19:39:57.631385"
  },
  "submission_detail": {
    "submission_id": "v3_commit_7500655_191",
    "miner_hotkey": "5DCsaYaL5VnvnMVRbuTojQFJafXVxuVucAbsPM9XQZATZp83",
    "miner_uid": 191,
    "code_hash": "https://gist.githubusercontent.com/algo-experter/e4b70cc33d6399e0373a416a0495da2f/raw",
    "bucket_path": "https://gist.githubusercontent.com/algo-experter/e4b70cc33d6399e0373a416a0495da2f/raw",
    "spec_version": 3,
    "status": "finished",
    "created_at": "2026-02-08 19:39:57.631385",
    "updated_at": "2026-02-08 23:12:17.499794",
    "final_score": 44.444478109007605,
    "error_message": null,
    "code_content": "\"\"\"\nBasic training implementation - Miners can optimize this!\n\nUsage:\n    1. Run setup: uv run local_test/setup_benchmark.py\n    2. Test locally: uv run local_test/train.py\n    3. Submit when ready!\n\"\"\"\n\nimport json\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM\n\n\n@dataclass\nclass InnerStepsResult:\n    \"\"\"Required return type from inner_steps function.\"\"\"\n\n    final_logits: torch.Tensor  # Output logits from last forward pass\n    total_tokens: int  # Total tokens processed across all steps\n    final_loss: float  # Loss value from last training step\n\n\n_COMPILED_MODELS: dict[int, torch.nn.Module] = {}\n_CUDA_STREAMS: dict[int, torch.cuda.Stream] = {}\n\n\ndef _get_execution_model(model: torch.nn.Module) -> torch.nn.Module:\n    \"\"\"Return a cached compiled wrapper when available.\n\n    The validator calls `inner_steps` twice per run (warmup + timed pass) with\n    the same model object. Caching the compiled wrapper by model id avoids\n    paying compile overhead during the timed pass.\n    \"\"\"\n    if not torch.cuda.is_available() or not hasattr(torch, \"compile\"):\n        return model\n\n    model_id = id(model)\n    compiled = _COMPILED_MODELS.get(model_id)\n    if compiled is not None:\n        return compiled\n\n    try:\n        compiled = torch.compile(\n            model,\n            mode=\"max-autotune\",\n            fullgraph=False,\n            dynamic=False,\n        )\n        _COMPILED_MODELS[model_id] = compiled\n        return compiled\n    except Exception:\n        return model\n\n\ndef _get_cuda_stream(device: torch.device) -> torch.cuda.Stream | None:\n    \"\"\"Return one reusable CUDA stream per device index for async host->device copies.\"\"\"\n    if device.type != \"cuda\" or not torch.cuda.is_available():\n        return None\n\n    index = device.index if device.index is not None else torch.cuda.current_device()\n    stream = _CUDA_STREAMS.get(index)\n    if stream is None:\n        stream = torch.cuda.Stream(device=index)\n        _CUDA_STREAMS[index] = stream\n    return stream\n\n\ndef inner_steps(model, data_iterator, optimizer, num_steps, device):\n    \"\"\"\n    Run training steps and return results.\n\n    Args:\n        model: Pre-loaded model (already on device, in train mode)\n        data_iterator: Iterator yielding batches of shape (batch_size, seq_len)\n        optimizer: Pre-configured optimizer\n        num_steps: Number of training steps to run\n        device: Target device (cuda or cpu)\n\n    Returns:\n        InnerStepsResult with outputs for verification\n    \"\"\"\n    # Keep behavior deterministic for validator verification.\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Keep math path aligned with validator reference, but enable fast kernels.\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    if hasattr(torch, \"set_float32_matmul_precision\"):\n        torch.set_float32_matmul_precision(\"high\")\n\n    run_model = _get_execution_model(model)\n    stream = _get_cuda_stream(device)\n    total_tokens = 0\n    final_logits = None\n    final_loss = 0.0\n\n    # Preload first batch.\n    batch = next(data_iterator)\n    if stream is not None:\n        with torch.cuda.stream(stream):\n            batch = batch.to(device, dtype=torch.long, non_blocking=True)\n        torch.cuda.current_stream().wait_stream(stream)\n    else:\n        batch = batch.to(device, dtype=torch.long, non_blocking=True)\n\n    for step in range(num_steps):\n        # Prepare inputs and labels (process ALL tokens - no truncation)\n        input_ids = batch[:, :-1].contiguous()\n        labels = batch[:, 1:].contiguous()\n\n        # Prefetch next batch while current step computes.\n        if step + 1 < num_steps:\n            next_batch = next(data_iterator)\n            if stream is not None:\n                with torch.cuda.stream(stream):\n                    next_batch = next_batch.to(device, dtype=torch.long, non_blocking=True)\n            else:\n                next_batch = next_batch.to(device, dtype=torch.long, non_blocking=True)\n        else:\n            next_batch = None\n\n        # Forward pass\n        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n            outputs = run_model(input_ids)\n            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n\n        # Compute loss\n        loss = F.cross_entropy(\n            logits.reshape(-1, logits.size(-1)),\n            labels.reshape(-1),\n            ignore_index=-100,\n        )\n\n        # Backward pass\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n\n        # Track metrics\n        total_tokens += batch.numel()\n        final_logits = logits.detach().float()\n        final_loss = loss.item()\n\n        if next_batch is not None:\n            if stream is not None:\n                torch.cuda.current_stream().wait_stream(stream)\n            batch = next_batch\n\n    return InnerStepsResult(\n        final_logits=final_logits,\n        total_tokens=total_tokens,\n        final_loss=final_loss,\n    )\n\n\n# =============================================================================\n# LOCAL TESTING - Run this file to test your implementation\n# =============================================================================\nif __name__ == \"__main__\":\n    print(\"=\" * 60)\n    print(\"TESTING train.py - Basic Implementation\")\n    print(\"=\" * 60)\n    print()\n\n    # Load configuration\n    hparams_path = Path(__file__).parent.parent / \"hparams\" / \"hparams.json\"\n    hparams = {}\n    if hparams_path.exists():\n        with open(hparams_path) as f:\n            hparams = json.load(f)\n\n    batch_size = hparams.get(\"benchmark_batch_size\", 16)\n    num_steps = hparams.get(\"eval_steps\", 5)\n    num_evals = hparams.get(\"evaluation_runs\", 5)\n\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Steps per eval: {num_steps}\")\n    print(f\"Evaluations: {num_evals}\")\n    print()\n\n    # Check paths\n    project_root = Path(__file__).parent.parent\n    model_path = project_root / \"benchmark\" / \"model\"\n    data_path = project_root / \"benchmark\" / \"data\" / \"train.pt\"\n\n    if not model_path.exists() or not data_path.exists():\n        print(\"Setup required! Run: uv run local_test/setup_benchmark.py\")\n        exit(1)\n\n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print()\n\n    # Load model\n    print(\"Loading model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n    model.gradient_checkpointing_enable()  # Required to fit in GPU memory\n    model.train()\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print()\n\n    # Load data\n    print(\"Loading data...\")\n    data = torch.load(data_path, weights_only=True)\n    print(f\"Samples: {data.shape[0]:,}, Sequence length: {data.shape[1]}\")\n    print()\n\n    # Create data iterator\n    def create_iterator():\n        idx = 0\n        while True:\n            end_idx = idx + batch_size\n            if end_idx > data.shape[0]:\n                idx = 0\n                end_idx = batch_size\n            yield data[idx:end_idx]\n            idx = end_idx\n\n    # Create optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    # Warmup\n    print(\"Warmup...\")\n    _ = inner_steps(model, create_iterator(), optimizer, num_steps=2, device=device)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n        torch.cuda.empty_cache()\n    print()\n\n    # Reset optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    # Run evaluations\n    print(f\"Running {num_evals} evaluations...\")\n\n    for i in range(num_evals):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        start = time.perf_counter()\n        result = inner_steps(model, create_iterator(), optimizer, num_steps, device)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        elapsed = time.perf_counter() - start\n        print(\n            f\"  Eval {i + 1}: {elapsed:.2f}s, tokens={result.total_tokens:,}, loss={result.final_loss:.4f}\"\n        )\n\n    print()\n    print(\"Done!\")",
    "payment_verified": 1
  },
  "evaluations": [
    {
      "evaluation_id": "aa1347f7-35fb-4531-8f31-bba96b08f9db",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 43.19866433625168,
      "tokens_per_second": 7279.245547619537,
      "total_tokens": 20480,
      "wall_time_seconds": 2.813478383992333,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 22:44:58.104677"
    },
    {
      "evaluation_id": "3005963a-ed24-4f30-ae7c-823df75fe66e",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 0.0,
      "tokens_per_second": 0.0,
      "total_tokens": 0,
      "wall_time_seconds": 2.715213580057025,
      "success": 0,
      "error": "Gradient relative error 0.051578 exceeds threshold 0.050000 (|g - g_truth| / |g_truth|)",
      "created_at": "2026-02-08 22:50:37.379513"
    },
    {
      "evaluation_id": "6412462d-ca65-4cbb-b408-937710092782",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 44.444478109007605,
      "tokens_per_second": 7489.172972409992,
      "total_tokens": 20480,
      "wall_time_seconds": 2.7346143660251983,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 22:53:31.791854"
    },
    {
      "evaluation_id": "df6bfb78-9ae8-4482-9c96-540d55e01733",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 0.0,
      "tokens_per_second": 0.0,
      "total_tokens": 0,
      "wall_time_seconds": 2.7208591899252497,
      "success": 0,
      "error": "Gradient relative error 0.057023 exceeds threshold 0.050000 (|g - g_truth| / |g_truth|)",
      "created_at": "2026-02-08 22:56:37.133943"
    },
    {
      "evaluation_id": "8d3a615c-c053-477a-95be-d7fdd0c0f6e0",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 44.49768784537834,
      "tokens_per_second": 7498.139146307219,
      "total_tokens": 20480,
      "wall_time_seconds": 2.7313443509628996,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 22:59:50.371790"
    },
    {
      "evaluation_id": "5f36850e-3132-423e-8910-0d09a72c1d63",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 44.45389412113022,
      "tokens_per_second": 7490.759629436849,
      "total_tokens": 20480,
      "wall_time_seconds": 2.734035133034922,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:02:57.275139"
    },
    {
      "evaluation_id": "4a4af3d1-a35e-4830-998f-8e73a06122a3",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 43.97089649899556,
      "tokens_per_second": 7409.371504492342,
      "total_tokens": 20480,
      "wall_time_seconds": 2.7640670990222134,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:05:50.086705"
    },
    {
      "evaluation_id": "36ccd08f-c8b8-4240-92d1-a66f375e254b",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 44.07527488544018,
      "tokens_per_second": 7426.959916459914,
      "total_tokens": 20480,
      "wall_time_seconds": 2.757521277933847,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:09:19.658006"
    },
    {
      "evaluation_id": "2dfd9c1a-d1c4-4d62-926f-6ad3dd6f27f5",
      "submission_id": "v3_commit_7500655_191",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 44.781635001875046,
      "tokens_per_second": 7545.9860208910995,
      "total_tokens": 20480,
      "wall_time_seconds": 2.7140257009887137,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:12:17.480888"
    }
  ],
  "fetched_at": "2026-02-09T19:50:57.539497"
}