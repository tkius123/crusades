{
  "rank": 4,
  "leaderboard_entry": {
    "rank": 4,
    "submission_id": "v4_commit_7514912_60",
    "miner_hotkey": "5CwY58RumJ2rkbf9CfUAEw6yFwfpDZ7fE3PW3fjTwf9SocZX",
    "miner_uid": 60,
    "final_score": 38.34126596671511,
    "num_evaluations": 5,
    "created_at": "2026-02-10 17:29:47.842240"
  },
  "submission_detail": {
    "submission_id": "v4_commit_7514912_60",
    "miner_hotkey": "5CwY58RumJ2rkbf9CfUAEw6yFwfpDZ7fE3PW3fjTwf9SocZX",
    "miner_uid": 60,
    "code_hash": "https://pastebin.com/raw/2YKpN0yJ",
    "bucket_path": "https://pastebin.com/raw/2YKpN0yJ",
    "spec_version": 4,
    "status": "finished",
    "created_at": "2026-02-10 17:29:47.842240",
    "updated_at": "2026-02-10 17:52:42.549501",
    "final_score": 38.34126596671511,
    "error_message": null,
    "code_content": "\"\"\"\r\nReference training implementation for Templar Crusades.\r\n\r\nThis is the baseline implementation. Miners should optimize it for maximum MFU\r\n(Model FLOPs Utilization) while passing all verification checks.\r\n\r\nUsage:\r\n    1. Run setup: uv run local_test/setup_benchmark.py\r\n    2. Test locally: uv run local_test/train.py\r\n    3. Verify locally: uv run local_test/verify.py\r\n    4. Submit this file (or your optimized version) as a GitHub Gist!\r\n\r\n=== SUBMISSION ===\r\n\r\nYou can submit this entire file as-is. The validator only calls the inner_steps\r\nfunction \u2014 the `if __name__ == \"__main__\":` block is for local testing and is\r\nignored during evaluation.\r\n\r\n=== VERIFICATION RULES ===\r\n\r\nYour inner_steps function MUST:\r\n  - Use the provided optimizer (call optimizer.step() and optimizer.zero_grad())\r\n  - Process ALL tokens in each batch (no truncation)\r\n  - Return actual final_logits tensor (not None)\r\n  - Return logits with correct shape: (batch_size, seq_len - 1, vocab_size)\r\n  - Produce gradients that closely match the reference implementation\r\n  - Train all model parameters (don't freeze layers)\r\n  - Call optimizer.step() for each training step\r\n\r\nYour inner_steps function MUST NOT:\r\n  - Access optimizer internals (e.g., optimizer.optimizer)\r\n  - Truncate or skip parts of input sequences\r\n  - Return None for final_logits\r\n  - Report inflated token counts\r\n  - Modify the model's requires_grad settings\r\n  - Modify torch backend settings (deterministic, benchmark, SDP toggles, etc.)\r\n\"\"\"\r\n\r\nimport json\r\nimport time\r\nfrom dataclasses import dataclass\r\nfrom pathlib import Path\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom transformers import AutoConfig, AutoModelForCausalLM\r\n\r\n\r\n@dataclass\r\nclass InnerStepsResult:\r\n    \"\"\"Required return type from inner_steps function.\r\n\r\n    All fields are verified by the validator:\r\n    - final_logits: Must be a 3D tensor (batch, seq_len-1, vocab), NOT None\r\n    - total_tokens: Should equal batch_size * seq_len * num_steps\r\n    - final_loss: Must be a positive float, close to reference loss\r\n    \"\"\"\r\n\r\n    final_logits: torch.Tensor  # Output logits from last forward pass\r\n    total_tokens: int  # Total tokens processed across all steps\r\n    final_loss: float  # Loss value from last training step\r\n\r\n\r\ndef inner_steps(model, data_iterator, optimizer, num_steps, device):\r\n    \"\"\"\r\n    Run training steps and return results.\r\n\r\n    This is the function the validator calls. It receives:\r\n    - model: Pre-loaded model (already on device, in train mode, with gradient checkpointing)\r\n    - data_iterator: Infinite iterator yielding batches of shape (batch_size, seq_len)\r\n    - optimizer: Pre-configured AdamW optimizer (wrapped by validator for gradient capture)\r\n    - num_steps: Number of training steps to run (must complete all of them)\r\n    - device: Target device (cuda or cpu)\r\n\r\n    The validator measures wall_time of this function and calculates:\r\n        MFU = (6 * model_params * batch_size * seq_len * num_steps) / (wall_time * gpu_peak_tflops)\r\n\r\n    Higher MFU = you completed the same training faster = better score.\r\n\r\n    Returns:\r\n        InnerStepsResult with outputs for verification\r\n    \"\"\"\r\n    total_tokens = 0\r\n    final_logits = None\r\n    final_loss = 0.0\r\n\r\n    for step in range(num_steps):\r\n        # Get batch - shape: (batch_size, seq_len)\r\n        batch = next(data_iterator)\r\n        batch = batch.to(device, dtype=torch.long)\r\n\r\n        # Prepare inputs and labels (causal LM: predict next token)\r\n        # input_ids: all tokens except last, labels: all tokens except first\r\n        input_ids = batch[:, :-1]\r\n        labels = batch[:, 1:]\r\n\r\n        # Forward pass\r\n        outputs = model(input_ids)\r\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\r\n\r\n        # Compute loss\r\n        loss = F.cross_entropy(\r\n            logits.reshape(-1, logits.size(-1)),\r\n            labels.reshape(-1),\r\n            ignore_index=-100,\r\n        )\r\n\r\n        # Backward pass\r\n        loss.backward()\r\n\r\n        # Update weights - MUST use the provided optimizer\r\n        optimizer.step()\r\n        optimizer.zero_grad(set_to_none=True)\r\n\r\n        # Track metrics\r\n        total_tokens += batch.numel()\r\n        # Keep logits from the last step for verification\r\n        final_logits = logits.detach().float()\r\n        final_loss = loss.item()\r\n\r\n    return InnerStepsResult(\r\n        final_logits=final_logits,\r\n        total_tokens=total_tokens,\r\n        final_loss=final_loss,\r\n    )",
    "payment_verified": 1
  },
  "evaluations": [
    {
      "evaluation_id": "d71dff20-c933-404e-9920-2bfed1b13883",
      "submission_id": "v4_commit_7514912_60",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.335500972942896,
      "tokens_per_second": 6459.772056861522,
      "total_tokens": 20480,
      "wall_time_seconds": 3.170390506000331,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:43:43.646575"
    },
    {
      "evaluation_id": "b7466463-6ee7-4609-9421-00a712c71e0f",
      "submission_id": "v4_commit_7514912_60",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.34126596671511,
      "tokens_per_second": 6460.743494425466,
      "total_tokens": 20480,
      "wall_time_seconds": 3.1699138059993857,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:45:57.246260"
    },
    {
      "evaluation_id": "728ab8b9-484c-4de4-b24b-bebe95ad5b73",
      "submission_id": "v4_commit_7514912_60",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.343357224313564,
      "tokens_per_second": 6461.0958844309525,
      "total_tokens": 20480,
      "wall_time_seconds": 3.169740917999661,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:48:11.063126"
    },
    {
      "evaluation_id": "aa6b19d4-a9e0-4078-8b21-65f50f4f40b7",
      "submission_id": "v4_commit_7514912_60",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.363176450231556,
      "tokens_per_second": 6464.435548150595,
      "total_tokens": 20480,
      "wall_time_seconds": 3.168103363001137,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:50:24.429631"
    },
    {
      "evaluation_id": "97ca63b7-1d40-4b4a-934d-9a4b741d3862",
      "submission_id": "v4_commit_7514912_60",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.28442380281543,
      "tokens_per_second": 6451.1652337352025,
      "total_tokens": 20480,
      "wall_time_seconds": 3.1746202829999675,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:52:42.529311"
    }
  ],
  "fetched_at": "2026-02-10T18:44:32.164452"
}