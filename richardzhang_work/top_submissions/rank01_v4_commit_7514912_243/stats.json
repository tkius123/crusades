{
  "rank": 1,
  "leaderboard_entry": {
    "rank": 1,
    "submission_id": "v4_commit_7514912_243",
    "miner_hotkey": "5GUEcgJ5reLfJ5KUbbQtFtLkFqLuqLc3jLBs3E7Ggok8rqX6",
    "miner_uid": 243,
    "final_score": 38.31836276072644,
    "num_evaluations": 5,
    "created_at": "2026-02-10 17:29:47.834609"
  },
  "submission_detail": {
    "submission_id": "v4_commit_7514912_243",
    "miner_hotkey": "5GUEcgJ5reLfJ5KUbbQtFtLkFqLuqLc3jLBs3E7Ggok8rqX6",
    "miner_uid": 243,
    "code_hash": "https://pastebin.com/raw/DRJieXxa",
    "bucket_path": "https://pastebin.com/raw/DRJieXxa",
    "spec_version": 4,
    "status": "finished",
    "created_at": "2026-02-10 17:29:47.834609",
    "updated_at": "2026-02-10 17:41:27.940810",
    "final_score": 38.31836276072644,
    "error_message": null,
    "code_content": "\"\"\"\r\nReference training implementation for Templar Crusades.\r\n\r\nThis is the baseline implementation. Miners should optimize it for maximum MFU\r\n(Model FLOPs Utilization) while passing all verification checks.\r\n\r\nUsage:\r\n    1. Run setup: uv run local_test/setup_benchmark.py\r\n    2. Test locally: uv run local_test/train.py\r\n    3. Verify locally: uv run local_test/verify.py\r\n    4. Submit this file (or your optimized version) as a GitHub Gist!\r\n\r\n=== SUBMISSION ===\r\n\r\nYou can submit this entire file as-is. The validator only calls the inner_steps\r\nfunction \u2014 the `if __name__ == \"__main__\":` block is for local testing and is\r\nignored during evaluation.\r\n\r\n=== VERIFICATION RULES ===\r\n\r\nYour inner_steps function MUST:\r\n  - Use the provided optimizer (call optimizer.step() and optimizer.zero_grad())\r\n  - Process ALL tokens in each batch (no truncation)\r\n  - Return actual final_logits tensor (not None)\r\n  - Return logits with correct shape: (batch_size, seq_len - 1, vocab_size)\r\n  - Produce gradients that closely match the reference implementation\r\n  - Train all model parameters (don't freeze layers)\r\n  - Call optimizer.step() for each training step\r\n\r\nYour inner_steps function MUST NOT:\r\n  - Access optimizer internals (e.g., optimizer.optimizer)\r\n  - Truncate or skip parts of input sequences\r\n  - Return None for final_logits\r\n  - Report inflated token counts\r\n  - Modify the model's requires_grad settings\r\n  - Modify torch backend settings (deterministic, benchmark, SDP toggles, etc.)\r\n\"\"\"\r\n\r\nimport json\r\nimport time\r\nfrom dataclasses import dataclass\r\nfrom pathlib import Path\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom transformers import AutoConfig, AutoModelForCausalLM\r\n\r\n\r\n@dataclass\r\nclass InnerStepsResult:\r\n    \"\"\"Required return type from inner_steps function.\r\n\r\n    All fields are verified by the validator:\r\n    - final_logits: Must be a 3D tensor (batch, seq_len-1, vocab), NOT None\r\n    - total_tokens: Should equal batch_size * seq_len * num_steps\r\n    - final_loss: Must be a positive float, close to reference loss\r\n    \"\"\"\r\n\r\n    final_logits: torch.Tensor  # Output logits from last forward pass\r\n    total_tokens: int  # Total tokens processed across all steps\r\n    final_loss: float  # Loss value from last training step\r\n\r\n\r\ndef inner_steps(model, data_iterator, optimizer, num_steps, device):\r\n    \"\"\"\r\n    Run training steps and return results.\r\n\r\n    This is the function the validator calls. It receives:\r\n    - model: Pre-loaded model (already on device, in train mode, with gradient checkpointing)\r\n    - data_iterator: Infinite iterator yielding batches of shape (batch_size, seq_len)\r\n    - optimizer: Pre-configured AdamW optimizer (wrapped by validator for gradient capture)\r\n    - num_steps: Number of training steps to run (must complete all of them)\r\n    - device: Target device (cuda or cpu)\r\n\r\n    The validator measures wall_time of this function and calculates:\r\n        MFU = (6 * model_params * batch_size * seq_len * num_steps) / (wall_time * gpu_peak_tflops)\r\n\r\n    Higher MFU = you completed the same training faster = better score.\r\n\r\n    Returns:\r\n        InnerStepsResult with outputs for verification\r\n    \"\"\"\r\n    total_tokens = 0\r\n    final_logits = None\r\n    final_loss = 0.0\r\n\r\n    for step in range(num_steps):\r\n        # Get batch - shape: (batch_size, seq_len)\r\n        batch = next(data_iterator)\r\n        batch = batch.to(device, dtype=torch.long)\r\n\r\n        # Prepare inputs and labels (causal LM: predict next token)\r\n        # input_ids: all tokens except last, labels: all tokens except first\r\n        input_ids = batch[:, :-1]\r\n        labels = batch[:, 1:]\r\n\r\n        # Forward pass\r\n        outputs = model(input_ids)\r\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\r\n\r\n        # Compute loss\r\n        loss = F.cross_entropy(\r\n            logits.reshape(-1, logits.size(-1)),\r\n            labels.reshape(-1),\r\n            ignore_index=-100,\r\n        )\r\n\r\n        # Backward pass\r\n        loss.backward()\r\n\r\n        # Update weights - MUST use the provided optimizer\r\n        optimizer.step()\r\n        optimizer.zero_grad(set_to_none=True)\r\n\r\n        # Track metrics\r\n        total_tokens += batch.numel()\r\n        # Keep logits from the last step for verification\r\n        final_logits = logits.detach().float()\r\n        final_loss = loss.item()\r\n\r\n    return InnerStepsResult(\r\n        final_logits=final_logits,\r\n        total_tokens=total_tokens,\r\n        final_loss=final_loss,\r\n    )\r\n\r\n\r\n# =============================================================================\r\n# LOCAL TESTING - Run this file to test your implementation\r\n# =============================================================================\r\nif __name__ == \"__main__\":\r\n    print(\"=\" * 60)\r\n    print(\"TESTING train.py - Basic Implementation\")\r\n    print(\"=\" * 60)\r\n    print()\r\n\r\n    # Load configuration\r\n    hparams_path = Path(__file__).parent.parent / \"hparams\" / \"hparams.json\"\r\n    hparams = {}\r\n    if hparams_path.exists():\r\n        with open(hparams_path) as f:\r\n            hparams = json.load(f)\r\n\r\n    batch_size = hparams.get(\"benchmark_batch_size\", 16)\r\n    num_steps = hparams.get(\"eval_steps\", 5)\r\n    num_evals = hparams.get(\"evaluation_runs\", 5)\r\n\r\n    print(f\"Batch size: {batch_size}\")\r\n    print(f\"Steps per eval: {num_steps}\")\r\n    print(f\"Evaluations: {num_evals}\")\r\n    print()\r\n\r\n    # Check paths\r\n    project_root = Path(__file__).parent.parent\r\n    model_path = project_root / \"benchmark\" / \"model\"\r\n    data_path = project_root / \"benchmark\" / \"data\" / \"train.pt\"\r\n\r\n    if not model_path.exists() or not data_path.exists():\r\n        print(\"Setup required! Run: uv run local_test/setup_benchmark.py\")\r\n        exit(1)\r\n\r\n    # Setup device\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    print(f\"Device: {device}\")\r\n    if torch.cuda.is_available():\r\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\r\n    print()\r\n\r\n    # Load model with RANDOM initialization (same as validator)\r\n    print(\"Loading model (random init, same as validator)...\")\r\n    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\n    model = AutoModelForCausalLM.from_config(\r\n        config,\r\n        torch_dtype=torch.bfloat16,\r\n        trust_remote_code=True,\r\n        attn_implementation=\"sdpa\",\r\n    )\r\n    model = model.to(device)\r\n    model.gradient_checkpointing_enable()  # Required to fit in GPU memory\r\n    model.train()\r\n    for param in model.parameters():\r\n        param.requires_grad = True\r\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\r\n    print()\r\n\r\n    # Load data\r\n    print(\"Loading data...\")\r\n    data = torch.load(data_path, weights_only=True)\r\n    print(f\"Samples: {data.shape[0]:,}, Sequence length: {data.shape[1]}\")\r\n    print()\r\n\r\n    # Create data iterator\r\n    def create_iterator():\r\n        idx = 0\r\n        while True:\r\n            end_idx = idx + batch_size\r\n            if end_idx > data.shape[0]:\r\n                idx = 0\r\n                end_idx = batch_size\r\n            yield data[idx:end_idx]\r\n            idx = end_idx\r\n\r\n    # Create optimizer (same config as validator)\r\n    use_fused = torch.cuda.is_available()\r\n    optimizer = torch.optim.AdamW(\r\n        model.parameters(), lr=1e-4, weight_decay=0.1, betas=(0.9, 0.95), fused=use_fused\r\n    )\r\n\r\n    # Warmup\r\n    print(\"Warmup...\")\r\n    _ = inner_steps(model, create_iterator(), optimizer, num_steps=2, device=device)\r\n    if torch.cuda.is_available():\r\n        torch.cuda.synchronize()\r\n        torch.cuda.empty_cache()\r\n    print()\r\n\r\n    # Reset optimizer (same config as validator)\r\n    optimizer = torch.optim.AdamW(\r\n        model.parameters(), lr=1e-4, weight_decay=0.1, betas=(0.9, 0.95), fused=use_fused\r\n    )\r\n\r\n    # Run evaluations\r\n    print(f\"Running {num_evals} evaluations...\")\r\n\r\n    for i in range(num_evals):\r\n        if torch.cuda.is_available():\r\n            torch.cuda.synchronize()\r\n\r\n        start = time.perf_counter()\r\n        result = inner_steps(model, create_iterator(), optimizer, num_steps, device)\r\n\r\n        if torch.cuda.is_available():\r\n            torch.cuda.synchronize()\r\n\r\n        elapsed = time.perf_counter() - start\r\n        print(\r\n            f\"  Eval {i + 1}: {elapsed:.2f}s, tokens={result.total_tokens:,}, loss={result.final_loss:.4f}\"\r\n        )\r\n\r\n    print()\r\n    print(\"Done!\")",
    "payment_verified": 1
  },
  "evaluations": [
    {
      "evaluation_id": "011b57eb-79f3-4073-87de-7e1acbac0482",
      "submission_id": "v4_commit_7514912_243",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.30969454820975,
      "tokens_per_second": 6455.423512636253,
      "total_tokens": 20480,
      "wall_time_seconds": 3.172526165000818,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:32:11.737103"
    },
    {
      "evaluation_id": "82666b21-fd49-49d5-a99b-528d4c4e50a5",
      "submission_id": "v4_commit_7514912_243",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.31836276072644,
      "tokens_per_second": 6456.88416074511,
      "total_tokens": 20480,
      "wall_time_seconds": 3.1718084900003305,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:34:29.357929"
    },
    {
      "evaluation_id": "6822512b-14d0-4712-8f76-ba3441f766de",
      "submission_id": "v4_commit_7514912_243",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.33610699577041,
      "tokens_per_second": 6459.874175504232,
      "total_tokens": 20480,
      "wall_time_seconds": 3.1703403880001133,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:36:54.298019"
    },
    {
      "evaluation_id": "219ad252-e4c3-4e27-a658-166ec9052e46",
      "submission_id": "v4_commit_7514912_243",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.28392607635981,
      "tokens_per_second": 6451.081363709552,
      "total_tokens": 20480,
      "wall_time_seconds": 3.174661556000501,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:39:09.981307"
    },
    {
      "evaluation_id": "df191ebb-56b5-4041-8986-b98e94455ba2",
      "submission_id": "v4_commit_7514912_243",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 38.337307199516914,
      "tokens_per_second": 6460.076417353887,
      "total_tokens": 20480,
      "wall_time_seconds": 3.170241136000186,
      "success": 1,
      "error": null,
      "created_at": "2026-02-10 17:41:27.919921"
    }
  ],
  "fetched_at": "2026-02-10T18:11:18.105455"
}