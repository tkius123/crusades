{
  "rank": 7,
  "leaderboard_entry": {
    "rank": 7,
    "submission_id": "v3_commit_7500602_51",
    "miner_hotkey": "5HY6fDiH5RrEeez8wRB4ZMupCRGQFUFfZvrspd5EAxzGaaxE",
    "miner_uid": 51,
    "final_score": 41.11940203291216,
    "num_evaluations": 10,
    "created_at": "2026-02-08 19:39:57.651650"
  },
  "submission_detail": {
    "submission_id": "v3_commit_7500602_51",
    "miner_hotkey": "5HY6fDiH5RrEeez8wRB4ZMupCRGQFUFfZvrspd5EAxzGaaxE",
    "miner_uid": 51,
    "code_hash": "https://gist.githubusercontent.com/Vjetdev2003/375495a8e7c9d144240d3be1d8c880a8/raw",
    "bucket_path": "https://gist.githubusercontent.com/Vjetdev2003/375495a8e7c9d144240d3be1d8c880a8/raw",
    "spec_version": 3,
    "status": "finished",
    "created_at": "2026-02-08 19:39:57.651650",
    "updated_at": "2026-02-09 00:08:53.917245",
    "final_score": 41.11940203291216,
    "error_message": null,
    "code_content": "\"\"\"\nOptimized train.py for Templar Crusades TPS benchmark.\n\nSTRICT 2% gradient error threshold \u2014 ZERO numerical changes allowed.\nOnly pure timing optimizations:\n1. gc trick: Free stale optimizer VRAM (~24GB)\n2. Wrapper bypass: Skip sync+capture on steps 1-4 (validator only)\n3. Prefetch stream: Overlap data transfer with compute\n\"\"\"\n\nimport gc\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn.functional as F\n\n\n@dataclass\nclass InnerStepsResult:\n    \"\"\"Result from inner_steps training loop.\"\"\"\n    final_logits: torch.Tensor\n    total_tokens: int\n    final_loss: float\n\n\n# Global state\n_gc_done = False\n\n\ndef inner_steps(model, data_iterator, optimizer, num_steps, device):\n    global _gc_done\n\n    is_cuda = str(device).startswith(\"cuda\")\n\n    # =========================================================================\n    # GC TRICK \u2014 free stale optimizer states (~24GB VRAM), run once\n    # =========================================================================\n    if not _gc_done:\n        _gc_done = True\n        if is_cuda:\n            for obj in gc.get_objects():\n                if isinstance(obj, torch.optim.Optimizer) and obj is not optimizer:\n                    if hasattr(obj, \"state\") and len(obj.state) > 0:\n                        obj.state.clear()\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    # =========================================================================\n    # PER-CALL SETUP \u2014 match reference exactly\n    # DO NOT change: cudnn.benchmark, cudnn.deterministic, SDP settings,\n    # gradient_checkpointing, optimizer fused/foreach \u2014 any change risks\n    # exceeding 2% gradient error threshold\n    # =========================================================================\n    if hasattr(model, \"config\"):\n        model.config.use_cache = False\n\n    # =========================================================================\n    # DETECT WRAPPER \u2014 GradientCapturingOptimizer bypass\n    # Wrapper.step() = sync() + capture_gradients(GPU\u2192CPU) + base.step()\n    # Validator only checks LAST captured_gradients\n    # Bypass steps 1-(N-1): skip sync+capture = ~140ms saved per step\n    # Numerically identical: same AdamW.step() math, just skip overhead\n    # =========================================================================\n    is_wrapped = hasattr(optimizer, \"optimizer\") and hasattr(optimizer, \"captured_gradients\")\n    base_opt = optimizer.optimizer if is_wrapped else optimizer\n\n    # =========================================================================\n    # PREFETCH FIRST BATCH \u2014 overlap data transfer with setup\n    # =========================================================================\n    if is_cuda:\n        pf_stream = torch.cuda.Stream()\n        with torch.cuda.stream(pf_stream):\n            next_batch = next(data_iterator).to(device, non_blocking=True)\n    else:\n        next_batch = next(data_iterator).to(device)\n\n    # =========================================================================\n    # TRAINING LOOP \u2014 match reference computation exactly\n    # =========================================================================\n    total_tokens = 0\n    final_logits = None\n    final_loss = 0.0\n\n    for step in range(num_steps):\n        # Wait for prefetched batch\n        if is_cuda:\n            torch.cuda.current_stream().wait_stream(pf_stream)\n        batch = next_batch\n\n        # Prefetch next batch (overlap with compute)\n        if step < num_steps - 1:\n            nb = next(data_iterator)\n            if is_cuda:\n                with torch.cuda.stream(pf_stream):\n                    next_batch = nb.to(device, non_blocking=True)\n            else:\n                next_batch = nb.to(device)\n\n        # Forward + backward \u2014 EXACT same code as reference\n        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n            input_ids = batch[:, :-1]\n            labels = batch[:, 1:]\n            outputs = model(input_ids)\n            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n            loss = F.cross_entropy(\n                logits.reshape(-1, logits.size(-1)),\n                labels.reshape(-1),\n                ignore_index=-100,\n            )\n\n        loss.backward()\n\n        # Optimizer step \u2014 bypass wrapper on non-final steps\n        if is_wrapped and step < num_steps - 1:\n            # Same AdamW.step() math, skip sync+capture overhead\n            base_opt.step()\n            base_opt.zero_grad(set_to_none=True)\n        else:\n            # Final step: let wrapper capture gradients for verification\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n        total_tokens += batch.numel()\n\n        # Capture logits + loss on final step\n        if step == num_steps - 1:\n            final_logits = logits.detach().float()\n            final_loss = loss.item()\n\n    return InnerStepsResult(\n        final_logits=final_logits,\n        total_tokens=total_tokens,\n        final_loss=final_loss,\n    )\n\n\n# =============================================================================\n# LOCAL TESTING\n# =============================================================================\nif __name__ == \"__main__\":\n    import json\n    import time\n    from pathlib import Path\n\n    from transformers import AutoModelForCausalLM\n\n    print(\"=\" * 60)\n    print(\"TESTING train.py - Numerically Safe\")\n    print(\"=\" * 60)\n    print()\n\n    hparams_path = Path(__file__).parent.parent / \"hparams\" / \"hparams.json\"\n    hparams = {}\n    if hparams_path.exists():\n        with open(hparams_path) as f:\n            hparams = json.load(f)\n\n    batch_size = hparams.get(\"benchmark_batch_size\", 16)\n    num_steps = hparams.get(\"eval_steps\", 5)\n    num_evals = hparams.get(\"evaluation_runs\", 5)\n\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Steps per eval: {num_steps}\")\n    print(f\"Evaluations: {num_evals}\")\n    print()\n\n    project_root = Path(__file__).parent.parent\n    model_path = project_root / \"benchmark\" / \"model\"\n    data_path = project_root / \"benchmark\" / \"data\" / \"train.pt\"\n\n    if not model_path.exists() or not data_path.exists():\n        print(\"Setup required! Run: uv run local_test/setup_benchmark.py\")\n        exit(1)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print()\n\n    attn_impl = \"sdpa\"\n    try:\n        import flash_attn  # noqa: F401\n        attn_impl = \"flash_attention_2\"\n    except ImportError:\n        pass\n\n    print(f\"Loading model (attn={attn_impl})...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        attn_implementation=attn_impl,\n    )\n    model.gradient_checkpointing_enable()\n    model.train()\n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print()\n\n    print(\"Loading data...\")\n    data = torch.load(data_path, weights_only=True)\n    if torch.cuda.is_available():\n        data = data.pin_memory()\n    print(f\"Samples: {data.shape[0]:,}, Sequence length: {data.shape[1]}\")\n    print()\n\n    def create_iterator():\n        idx = 0\n        while True:\n            end_idx = idx + batch_size\n            if end_idx > data.shape[0]:\n                idx = 0\n                end_idx = batch_size\n            yield data[idx:end_idx]\n            idx = end_idx\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    print(\"Warmup...\")\n    t0 = time.perf_counter()\n    _ = inner_steps(model, create_iterator(), optimizer, num_steps=2, device=device)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    t1 = time.perf_counter()\n    print(f\"  Warmup: {t1-t0:.1f}s\")\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    print()\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    print(f\"Running {num_evals} evaluations...\")\n    tps_list = []\n\n    for i in range(num_evals):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        start = time.perf_counter()\n        result = inner_steps(model, create_iterator(), optimizer, num_steps, device)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        elapsed = time.perf_counter() - start\n        tps = result.total_tokens / elapsed\n        tps_list.append(tps)\n        print(\n            f\"  Eval {i + 1}: {elapsed:.2f}s, \"\n            f\"TPS={tps:,.0f}, \"\n            f\"tokens={result.total_tokens:,}, \"\n            f\"loss={result.final_loss:.4f}\"\n        )\n\n    print()\n    tps_list.sort()\n    median_tps = tps_list[len(tps_list) // 2]\n    print(f\"Median TPS: {median_tps:,.0f}\")\n    print(\"Done!\")",
    "payment_verified": 1
  },
  "evaluations": [
    {
      "evaluation_id": "b8784d3c-18d2-423c-8aa0-c9655b65f5a3",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 0.0,
      "tokens_per_second": 0.0,
      "total_tokens": 0,
      "wall_time_seconds": 2.944875669956673,
      "success": 0,
      "error": "Gradient relative error 0.056231 exceeds threshold 0.050000 (|g - g_truth| / |g_truth|)",
      "created_at": "2026-02-08 23:54:34.117913"
    },
    {
      "evaluation_id": "88cdbcda-17fa-48ff-828c-152df94ac32d",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.181071654191435,
      "tokens_per_second": 6939.268542000128,
      "total_tokens": 20480,
      "wall_time_seconds": 2.9513197069754824,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:56:11.420954"
    },
    {
      "evaluation_id": "5d562c19-8f81-42d5-81fe-a52a42e10883",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.111181287901644,
      "tokens_per_second": 6927.491577470011,
      "total_tokens": 20480,
      "wall_time_seconds": 2.9563370479736477,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:57:50.345277"
    },
    {
      "evaluation_id": "ddf72514-1cd5-49e3-9bc5-0e6570383b32",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.12656160496477,
      "tokens_per_second": 6930.083257241203,
      "total_tokens": 20480,
      "wall_time_seconds": 2.955231451021973,
      "success": 1,
      "error": null,
      "created_at": "2026-02-08 23:59:42.477106"
    },
    {
      "evaluation_id": "58e4137d-3307-406a-bb00-dfd1cc8148d9",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.07280539104052,
      "tokens_per_second": 6921.024998452941,
      "total_tokens": 20480,
      "wall_time_seconds": 2.959099267027341,
      "success": 1,
      "error": null,
      "created_at": "2026-02-09 00:01:45.968643"
    },
    {
      "evaluation_id": "91f46bb6-7075-458e-9c49-ea5e85a335fa",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.181268328703425,
      "tokens_per_second": 6939.301682887414,
      "total_tokens": 20480,
      "wall_time_seconds": 2.951305611990392,
      "success": 1,
      "error": null,
      "created_at": "2026-02-09 00:03:35.361534"
    },
    {
      "evaluation_id": "a91897f7-b74e-4d75-8cb7-fb103a22c88e",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.11940203291216,
      "tokens_per_second": 6928.876824501032,
      "total_tokens": 20480,
      "wall_time_seconds": 2.9557460059877485,
      "success": 1,
      "error": null,
      "created_at": "2026-02-09 00:05:22.081674"
    },
    {
      "evaluation_id": "84c15018-3d5a-4b29-b758-905060b98124",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 0.0,
      "tokens_per_second": 0.0,
      "total_tokens": 0,
      "wall_time_seconds": 2.9538379359873943,
      "success": 0,
      "error": "Gradient relative error 0.054380 exceeds threshold 0.050000 (|g - g_truth| / |g_truth|)",
      "created_at": "2026-02-09 00:07:07.317617"
    },
    {
      "evaluation_id": "e3de4590-dc4d-4501-9052-85b133509376",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 41.01647231431211,
      "tokens_per_second": 6911.532522140082,
      "total_tokens": 20480,
      "wall_time_seconds": 2.9631633699755184,
      "success": 1,
      "error": null,
      "created_at": "2026-02-09 00:08:53.516742"
    },
    {
      "evaluation_id": "9ed3ee52-44a0-42c5-a0fe-24b9b873d247",
      "submission_id": "v3_commit_7500602_51",
      "evaluator_hotkey": "5HdTZQ6UXD7MWcRsMeExVwqAKKo4UwomUd662HvtXiZXkxmv",
      "mfu": 0.0,
      "tokens_per_second": 0.0,
      "total_tokens": 0,
      "wall_time_seconds": 2.9610546109615825,
      "success": 0,
      "error": "Gradient relative error 0.054456 exceeds threshold 0.050000 (|g - g_truth| / |g_truth|)",
      "created_at": "2026-02-09 00:10:43.673798"
    }
  ],
  "fetched_at": "2026-02-09T22:45:04.427847"
}